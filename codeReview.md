ê±´ì„¤ê³µì‚¬ ì‚¬ê³  ì˜ˆë°© ë° ëŒ€ì‘ì±… ìƒì„± : í•œì†”ë°ì½” ì‹œì¦Œ 3 AI ê²½ì§„ëŒ€íšŒğŸ‘·
===============================================================
---------------------------------------------------------------

### ê°œë°œí™˜ê²½
```
Colab pro A100 GPU
python
```

<hr/>

### ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
```
import pandas as pd
import tensorflow as tf
import torch

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline
from langchain_huggingface import HuggingFacePipeline

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
```

### PDF ë¬¸ì„œ ë¡œë“œ
- PyPDFium2Loader : langchain_community.document_loadersì—ì„œ ì œê³µí•˜ëŠ” PDF ë¬¸ì„œ ë¡œë”. PyPDFium2 ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ PDF ë¬¸ì„œë¥¼ ì½ê³  í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ê¸°ëŠ¥ ì œê³µ.
- pdf_path : PDF íŒŒì¼ì˜ ê²½ë¡œ ì €ì¥
- loader : PyPDFium2Loader ê°ì²´ë¥¼ ìƒì„±í•˜ê³ , pdf_pathë¥¼ ì „ë‹¬
- load : load() ë©”ì„œë“œë¥¼ ì‹¤í–‰í•˜ì—¬ PDF ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ ë¡œë“œ. ë°˜í™˜ê°’ loadëŠ” ì¼ë°˜ì ìœ¼ë¡œ Document ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ì´ë©°, ê° ê°ì²´ëŠ” PDF í˜ì´ì§€ ë˜ëŠ” ì„¹ì…˜ì„ ë‚˜íƒ€ëƒ„.
```
from langchain_community.document_loaders import PyPDFium2Loader

pdf_path = '/content/drive/MyDrive/Colab Notebooks/data./á„€á…¥á†«á„‰á…¥á†¯á„‹á…¡á†«á„Œá…¥á†«á„Œá…µá„á…µá†·/á„€á…¥á†«á„‰á…¥á†¯á„€á…©á†¼á„‰á…¡ á„‹á…¡á†«á„Œá…¥á†«á„‡á…©á„€á…¥á†« á„‰á…¥á†¯á„€á…¨ á„Œá…µá„á…µá†·.pdf'

loader = PyPDFium2Loader(pdf_path)
load = loader.load()
```

### ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
- ë°ì´í„° ì „ì²˜ë¦¬
1. ê³µì‚¬ ì¢…ë¥˜ ì»¬ëŸ¼ ë¶„í•  : ê³µì‚¬ì¢…ë¥˜ ì—´ì˜ ê°’ì„ '/' ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê¸° (ëŒ€ë¶„ë¥˜ or ì¤‘ë¶„ë¥˜)
2. ê³µì¢… ì»¬ëŸ¼ ë¶„í•  : ê³µì¢… ì—´ì„ '>' ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê¸° (ëŒ€ë¶„ë¥˜ or ì¤‘ë¶„ë¥˜)
3. ì‚¬ê³ ê°ì²´ ì»¬ëŸ¼ ë¶„í•  : ì‚¬ê³ ê°ì²´ ì—´ì„ '>' ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê¸° (ëŒ€ë¶„ë¥˜ or ì¤‘ë¶„ë¥˜)
```
train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data./train.csv', encoding = 'utf-8-sig')
test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data./test.csv', encoding = 'utf-8-sig')

train['ê³µì‚¬ì¢…ë¥˜(ëŒ€ë¶„ë¥˜)'] = train['ê³µì‚¬ì¢…ë¥˜'].str.split('/').str[0]
train['ê³µì‚¬ì¢…ë¥˜(ì¤‘ë¶„ë¥˜)'] = train['ê³µì‚¬ì¢…ë¥˜'].str.split('/').str[1]
train['ê³µì¢…(ëŒ€ë¶„ë¥˜)'] = train['ê³µì¢…'].str.split('>').str[0]
train['ê³µì¢…(ì¤‘ë¶„ë¥˜)'] = train['ê³µì¢…'].str.split('>').str[1]
train['ì‚¬ê³ ê°ì²´(ëŒ€ë¶„ë¥˜)'] = train['ì‚¬ê³ ê°ì²´'].str.split('>').str[0]
train['ì‚¬ê³ ê°ì²´(ì¤‘ë¶„ë¥˜)'] = train['ì‚¬ê³ ê°ì²´'].str.split('>').str[1]

test['ê³µì‚¬ì¢…ë¥˜(ëŒ€ë¶„ë¥˜)'] = test['ê³µì‚¬ì¢…ë¥˜'].str.split('/').str[0]
test['ê³µì‚¬ì¢…ë¥˜(ì¤‘ë¶„ë¥˜)'] = test['ê³µì‚¬ì¢…ë¥˜'].str.split('/').str[1]
test['ê³µì¢…(ëŒ€ë¶„ë¥˜)'] = test['ê³µì¢…'].str.split('>').str[0]
test['ê³µì¢…(ì¤‘ë¶„ë¥˜)'] = test['ê³µì¢…'].str.split('>').str[1]
test['ì‚¬ê³ ê°ì²´(ëŒ€ë¶„ë¥˜)'] = test['ì‚¬ê³ ê°ì²´'].str.split('>').str[0]
test['ì‚¬ê³ ê°ì²´(ì¤‘ë¶„ë¥˜)'] = test['ì‚¬ê³ ê°ì²´'].str.split('>').str[1]
```

### ë°ì´í„°í”„ë ˆì„ ê¸°ë°˜ QA í˜•ì‹ ë°ì´í„°ì…‹ ìƒì„±
1. í›ˆë ¨ ë°ì´í„°
- ê° í–‰ì— ëŒ€í•´ íŠ¹ì • í•„ë“œë¥¼ ì¡°í•©í•˜ì—¬ questionê³¼ answer êµ¬ì¡° ë§Œë“¤ê¸°
- question : ì‚¬ê³ ì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬
- answer : í•´ë‹¹ ì‚¬ê³ ì˜ 'ì¬ë°œë°©ì§€ëŒ€ì±… ë° í–¥í›„ì¡°ì¹˜ê³„íš' ì €ì¥
- combined_training_data : ì§ˆë¬¸ê³¼ ë‹µë³€ì´ í¬í•¨ëœ Pandas DataFrame í˜•íƒœë¡œ ë³€í™˜
```
combined_training_data = train.apply(
     lambda row: {
        "question": (
            f"ê³µì‚¬ì¢…ë¥˜ ëŒ€ë¶„ë¥˜ '{row['ê³µì‚¬ì¢…ë¥˜(ëŒ€ë¶„ë¥˜)']}', ì¤‘ë¶„ë¥˜ '{row['ê³µì‚¬ì¢…ë¥˜(ì¤‘ë¶„ë¥˜)']}' ê³µì‚¬ ì¤‘ "
            f"ê³µì¢… ëŒ€ë¶„ë¥˜ '{row['ê³µì¢…(ëŒ€ë¶„ë¥˜)']}', ì¤‘ë¶„ë¥˜ '{row['ê³µì¢…(ì¤‘ë¶„ë¥˜)']}' ì‘ì—…ì—ì„œ "
            f"ì‚¬ê³ ê°ì²´ '{row['ì‚¬ê³ ê°ì²´(ëŒ€ë¶„ë¥˜)']}'(ì¤‘ë¶„ë¥˜: '{row['ì‚¬ê³ ê°ì²´(ì¤‘ë¶„ë¥˜)']}')ì™€ ê´€ë ¨ëœ ì‚¬ê³ ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. "
            f"ì‘ì—… í”„ë¡œì„¸ìŠ¤ëŠ” '{row['ì‘ì—…í”„ë¡œì„¸ìŠ¤']}'ì´ë©°, ì‚¬ê³  ì›ì¸ì€ '{row['ì‚¬ê³ ì›ì¸']}'ì…ë‹ˆë‹¤. "
            f"ì¬ë°œ ë°©ì§€ ëŒ€ì±… ë° í–¥í›„ ì¡°ì¹˜ ê³„íšì€ ë¬´ì—‡ì¸ê°€ìš”?"
        ),
        "answer": row["ì¬ë°œë°©ì§€ëŒ€ì±… ë° í–¥í›„ì¡°ì¹˜ê³„íš"]
    },
    axis=1
)

# DataFrameìœ¼ë¡œ ë³€í™˜
combined_training_data = pd.DataFrame(list(combined_training_data))
```
   
2. í…ŒìŠ¤íŠ¸ ë°ì´í„°
- ê° í–‰ì— ëŒ€í•´ íŠ¹ì • í•„ë“œë¥¼ ì¡°í•©í•˜ì—¬ question êµ¬ì¡° ë§Œë“¤ê¸°
- question : ì‚¬ê³ ì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬
- combined_test_data : ì§ˆë¬¸ì´ í¬í•¨ëœ Pandas DataFrame í˜•íƒœë¡œ ë³€í™˜
```
combined_test_data = test.apply(
    lambda row: {
        "question": (
            f"ê³µì‚¬ì¢…ë¥˜ ëŒ€ë¶„ë¥˜ '{row['ê³µì‚¬ì¢…ë¥˜(ëŒ€ë¶„ë¥˜)']}', ì¤‘ë¶„ë¥˜ '{row['ê³µì‚¬ì¢…ë¥˜(ì¤‘ë¶„ë¥˜)']}' ê³µì‚¬ ì¤‘ "
            f"ê³µì¢… ëŒ€ë¶„ë¥˜ '{row['ê³µì¢…(ëŒ€ë¶„ë¥˜)']}', ì¤‘ë¶„ë¥˜ '{row['ê³µì¢…(ì¤‘ë¶„ë¥˜)']}' ì‘ì—…ì—ì„œ "
            f"ì‚¬ê³ ê°ì²´ '{row['ì‚¬ê³ ê°ì²´(ëŒ€ë¶„ë¥˜)']}'(ì¤‘ë¶„ë¥˜: '{row['ì‚¬ê³ ê°ì²´(ì¤‘ë¶„ë¥˜)']}')ì™€ ê´€ë ¨ëœ ì‚¬ê³ ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. "
            f"ì‘ì—… í”„ë¡œì„¸ìŠ¤ëŠ” '{row['ì‘ì—…í”„ë¡œì„¸ìŠ¤']}'ì´ë©°, ì‚¬ê³  ì›ì¸ì€ '{row['ì‚¬ê³ ì›ì¸']}'ì…ë‹ˆë‹¤. "
            f"ì¬ë°œ ë°©ì§€ ëŒ€ì±… ë° í–¥í›„ ì¡°ì¹˜ ê³„íšì€ ë¬´ì—‡ì¸ê°€ìš”?"
        )
    },
    axis=1
)

# DataFrameìœ¼ë¡œ ë³€í™˜
combined_test_data = pd.DataFrame(list(combined_test_data))
```

### ëª¨ë¸ ì„¤ì •
- BitsAndBytesConfig : Hugging Face transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” ì €ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •í´ë˜ìŠ¤(ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ, ì†ë„ í–¥ìƒ, ì„±ëŠ¥ ìœ ì§€ë¥¼ ëª©ì ìœ¼ë¡œ ì‚¬ìš©)
- load_in_4bit = True : ëª¨ë¸ì„ 4bit ì–‘ìí™”í•˜ì—¬ ë¡œë“œ(ëŒ€í˜• ëª¨ë¸ì„ ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ê¸° ìœ„í•¨).
- bnb_4bit_use_double_quant = True : ì´ì¤‘ ì–‘ìí™”(íš¨ìœ¨ì ì¸ ì••ì¶• ê°€ëŠ¥).
- bnb_4bit_quant_type = "nf4" : Normal Float 4(LLM ì„±ëŠ¥ ì €í•˜ë¥¼ ìµœì†Œí™”í•˜ë©´ì„œ ë©”ëª¨ë¦¬ ì ˆì•½ ê°€ëŠ¥).
- bnb_4bit_compute_dtype = torch.bfloat16 : NVIDIA A100, H100 ë“± ìµœì‹  GPUì— ìµœì í™”
```
bnb_config = BitsAndBytesConfig(
    load_in_4bit = True,
    bnb_4bit_use_double_quant = True,
    bnb_4bit_quant_type = "nf4",
    bnb_4bit_compute_dtype = torch.bfloat16
)
```

### LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct ëª¨ë¸
- LG AI Researchì—ì„œ ê°œë°œí•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸
- ëª¨ë¸ ì‚¬ì´ì¦ˆ : 32B, 7.8B, 2.4B
- ì¥ì  : ì €ì‚¬ì–‘ GPUì—ì„œë„ í›ˆë ¨ ë° ë°°í¬ ê°€ëŠ¥, long-context processing ê°€ëŠ¥, ìµœëŒ€ 32K í† í°ì˜ ê¸´ ë¬¸ë§¥ ì²˜ë¦¬ ê°€ëŠ¥
- í•œêµ­ì–´ ë° ì˜ì–´ ì§€ì›
```
from transformers import AutoTokenizer
from transformers import AutoModelForCausalLM

model_id = "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"

model = AutoModelForCausalLM.from_pretrained(model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id, quantization_config=bnb_config, device_map="auto")
```

### FAISS ê¸°ë°˜ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
1. í›ˆë ¨ ë°ì´í„° ì¤€ë¹„
- conbined_training_dataì—ì„œ ì§ˆë¬¸ê³¼ ë‹µë³€ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜´.
- tolist()ë¥¼ ì‚¬ìš©í•´ Pandas ì‹œë¦¬ì¦ˆë¥¼ Python ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜.
- ê° ì§ˆë¬¸ê³¼ ë‹µë³€ì„ Q&A í¬ë§·ì˜ í…ìŠ¤íŠ¸ ë¬¸ì„œë¡œ ë³€í™˜.
- zip(train_questions_prevention, train_answers_prevention) : ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ìŒìœ¼ë¡œ ì²˜ë¦¬
2. ì„ë² ë”© ëª¨ë¸ ì •ì˜
- jhgan/ko-sbert-nli : Sentence-BERT ê¸°ë°˜ì˜ í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸(=Hugging Faceì—ì„œ ì œê³µí•˜ëŠ” í•œê¸€ ë¬¸ì¥ ì˜ë¯¸ ë¶„ì„ ìµœì í™” ëª¨ë¸).
3. ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
- FAISS.from_texts() : í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë²¡í„°í™”í•˜ì—¬ FAISS ì €ì¥ì†Œì— ì¶”ê°€
- FAISS : ëŒ€ê·œëª¨ ë²¡í„° ê²€ìƒ‰ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬. ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ë¹ ë¥´ê²Œ ì°¾ê¸° ìœ„í•œ ë²¡í„° ì¸ë±ì‹± ì§€ì›.
- í›ˆë ¨ ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ FAISSì— ì €ì¥
4. Retriever ì •ì˜(ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰)
```
# vector store ìƒì„±
# Train ë°ì´í„° ì¤€ë¹„
train_questions_prevention = combined_training_data['question'].tolist()
train_answers_prevention = combined_training_data['answer'].tolist()

train_documents = [
    f"Q: {q1}\nA: {a1}"
    for q1, a1 in zip(train_questions_prevention, train_answers_prevention)
]

# ì„ë² ë”© ìƒì„±
embedding_model_name = "jhgan/ko-sbert-nli"  # ì„ë² ë”© ëª¨ë¸ ì„ íƒ
embedding = HuggingFaceEmbeddings(model_name=embedding_model_name)

# ë²¡í„° ìŠ¤í† ì–´ì— ë¬¸ì„œ ì¶”ê°€
vector_store = FAISS.from_texts(train_documents, embedding)

# Retriever ì •ì˜
retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 5})
```

### RAG ì²´ì¸ ìƒì„±
1. í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ ì„¤ì •
- pipeline() : Hugging Face transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸
- text-generation : í…ìŠ¤íŠ¸ ìƒì„±
- do_sample =  True : ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ í™•ë¥ ì ìœ¼ë¡œ ë‹¨ì–´ ì„ íƒ (ëœë¤ì„±)
- temperature = 0.1 : ë‚®ì€ ê°’ì¼ìˆ˜ë¡ ë” ê²°ì •ë¡ ì ì¸(ì •í™•í•œ) ë‹µë³€ ìƒì„±
- return_full_text = False : ì…ë ¥ í”„ë¡¬í”„íŠ¸ëŠ” ì¶œë ¥ì— í¬í•¨í•˜ì§€ ì•ŠìŒ
- max_new_tokens = 64 : ìµœëŒ€ 64ê°œ í† í°ê¹Œì§€ ì¶œë ¥
```
text_generation_pipeline = pipeline(
    model=model,
    tokenizer=tokenizer,
    task="text-generation",
    do_sample=True,  # sampling í™œì„±í™”
    temperature=0.1,
    return_full_text=False,
    max_new_tokens=64,
)
```
2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
```
prompt_template = """
### ì§€ì¹¨: ë‹¹ì‹ ì€ ê±´ì„¤ê³µì‚¬ ì‚¬ê³  ìƒí™© ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ê³  ì›ì¸ì„ ë¶„ì„í•˜ê³  ì¬ë°œë°©ì§€ ëŒ€ì±…ì„ í¬í•¨í•œ ëŒ€ì‘ì±…ì„ ìë™ìœ¼ë¡œ ìƒì„±í•˜ëŠ” AI ëª¨ë¸ì…ë‹ˆë‹¤.
ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ í•µì‹¬ ë‚´ìš©ë§Œ ìš”ì•½í•˜ì—¬ ê°„ëµí•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
- ì„œë¡ , ë°°ê²½ ì„¤ëª… ë˜ëŠ” ì¶”ê°€ ì„¤ëª…ì„ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
- 'ë‹¤ìŒê³¼ ê°™ì€ ì¡°ì¹˜ë¥¼ ì·¨í•  ê²ƒì„ ì œì•ˆí•©ë‹ˆë‹¤:' ì™€ ê°™ì€ ë‚´ìš©ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
- ì…ë ¥ë°›ì€ ë‚´ìš©ì— ëŒ€í•´ ì¶”ê°€, ì‚­ì œ, ë³€ê²½ ì—†ì´ ë°›ì€ ë‚´ìš©ë§Œì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.

{context}

### ì§ˆë¬¸:
{question}

[/INST]

"""
```
3. LLM ì—°ê²° + í”„ë¡¬í”„íŠ¸ ê°ì²´ ìƒì„±
- HuggingFacePipeline()ì„ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ì„ LLM ëª¨ë¸ë¡œ ë³€í™˜
- ì´í›„ ì²´ì¸ì—ì„œ LLM í˜¸ì¶œí•  ìˆ˜ ìˆë„ë¡ ë˜í•‘
- í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ LangChainì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê°ì²´í™”
```
llm = HuggingFacePipeline(pipeline=text_generation_pipeline)

# ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ìƒì„±
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=prompt_template,
)
```
4. RAG ì²´ì¸ ìƒì„±
- RetrievalQA.from_chain_type()ì„ ì‚¬ìš©í•´ ê²€ìƒ‰ ê¸°ë°˜ QA ì²´ì¸ ìƒì„±
- chain_type = 'stuff' : ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë‹¨ìˆœ ê²°í•©í•˜ì—¬ ë‹µë³€ ìƒì„±
- return_source_documents = True : ì°¸ê³  ë¬¸ì„œ(ì¶œì²˜) ë°˜í™˜
- chain_type_kwargs = {"prompt" : prompt} : ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ ì ìš©
```
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # ë‹¨ìˆœ ì»¨í…ìŠ¤íŠ¸ ê²°í•© ë°©ì‹ ì‚¬ìš©
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt}  # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ì ìš©
)
```

### AI ëª¨ë¸ ë‹µë³€ ìƒì„± ë° ì €ì¥
- batch : ë³‘ë ¬ ì‹¤í–‰. GPU ì‚¬ìš© ì‹œ ë³‘ë ¬ ì—°ì‚° ì´ˆì í™” ê°€ëŠ¥
- batch_size : ë°°ì¹˜ ë‹¨ìœ„ë¡œ ëª¨ë¸ì„ ì‹¤í–‰í•˜ì—¬ ì†ë„ + ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì¦ê°€
- qa_chain.batch(batch_questions) : ë°°ì¹˜ í¬ê¸°ë§Œí¼ í•œ ë²ˆì— ì—¬ëŸ¬ ê°œì˜ ì§ˆë¬¸ ì²˜ë¦¬
```
test_results = []

print("í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘... ì´ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜:", len(combined_test_data))

# ë°°ì¹˜ í¬ê¸° ì„¤ì • (ì˜ˆ: 16)
batch_size = 16

# ë°ì´í„° ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
for start_idx in range(0, len(combined_test_data), batch_size):
    end_idx = min(start_idx + batch_size, len(combined_test_data))
    batch_questions = combined_test_data['question'][start_idx:end_idx].tolist()

    print(f"\n[ìƒ˜í”Œ {start_idx + 1}~{end_idx}/{len(combined_test_data)}] ì§„í–‰ ì¤‘...")

    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ëª¨ë¸ í˜¸ì¶œ
    batch_results = qa_chain.batch(batch_questions)  # ğŸ”¥ ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡ batch ì‚¬ìš©

    # ê²°ê³¼ ì €ì¥
    batch_texts = [res['result'] for res in batch_results]
    test_results.extend(batch_texts)

print("\ní…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì™„ë£Œ! ì´ ê²°ê³¼ ìˆ˜:", len(test_results))
```

### AI ëª¨ë¸ í‰ê°€ (ë²¡í„°í™”)
- SentenceTransformer(embedding_model_name) : Hugging Faceì—ì„œ SBERT ëª¨ë¸ ë¡œë“œ
- jhgan/ko-sbert-sts : í•œêµ­ì–´ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë¶„ì„ì— ìµœì í™”ëœ SBERT ëª¨ë¸.
```
# submission
from sentence_transformers import SentenceTransformer

embedding_model_name = "jhgan/ko-sbert-sts"
embedding = SentenceTransformer(embedding_model_name)

# ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì—¬ ì„ë² ë”© ìƒì„±
pred_embeddings = embedding.encode(test_results)
print(pred_embeddings.shape)  # (ìƒ˜í”Œ ê°œìˆ˜, 768)
```
