ê±´ì„¤ê³µì‚¬ ì‚¬ê³  ì˜ˆë°© ë° ëŒ€ì‘ì±… ìƒì„± : í•œì†”ë°ì½” ì‹œì¦Œ 3 AI ê²½ì§„ëŒ€íšŒğŸ‘·
===============================================================
---------------------------------------------------------------

### ê°œë°œí™˜ê²½
```
Colab pro A100 GPU
python
```

<hr/>

### ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
```
import pandas as pd
import tensorflow as tf
import torch

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline
from langchain_huggingface import HuggingFacePipeline

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
```

### PDF ë¬¸ì„œ ë¡œë“œ
- PyPDFium2Loader : langchain_community.document_loadersì—ì„œ ì œê³µí•˜ëŠ” PDF ë¬¸ì„œ ë¡œë”. PyPDFium2 ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ PDF ë¬¸ì„œë¥¼ ì½ê³  í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ê¸°ëŠ¥ ì œê³µ.
- pdf_path : PDF íŒŒì¼ì˜ ê²½ë¡œ ì €ì¥
- loader : PyPDFium2Loader ê°ì²´ë¥¼ ìƒì„±í•˜ê³ , pdf_pathë¥¼ ì „ë‹¬
- load : load() ë©”ì„œë“œë¥¼ ì‹¤í–‰í•˜ì—¬ PDF ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ ë¡œë“œ. ë°˜í™˜ê°’ loadëŠ” ì¼ë°˜ì ìœ¼ë¡œ Document ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ì´ë©°, ê° ê°ì²´ëŠ” PDF í˜ì´ì§€ ë˜ëŠ” ì„¹ì…˜ì„ ë‚˜íƒ€ëƒ„.
```
from langchain_community.document_loaders import PyPDFium2Loader

pdf_path = '/content/drive/MyDrive/Colab Notebooks/data./á„€á…¥á†«á„‰á…¥á†¯á„‹á…¡á†«á„Œá…¥á†«á„Œá…µá„á…µá†·/á„€á…¥á†«á„‰á…¥á†¯á„€á…©á†¼á„‰á…¡ á„‹á…¡á†«á„Œá…¥á†«á„‡á…©á„€á…¥á†« á„‰á…¥á†¯á„€á…¨ á„Œá…µá„á…µá†·.pdf'

loader = PyPDFium2Loader(pdf_path)
load = loader.load()
```

### ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
- ë°ì´í„° ì „ì²˜ë¦¬
1. ê³µì‚¬ ì¢…ë¥˜ ì»¬ëŸ¼ ë¶„í•  : ê³µì‚¬ì¢…ë¥˜ ì—´ì˜ ê°’ì„ '/' ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê¸° (ëŒ€ë¶„ë¥˜ or ì¤‘ë¶„ë¥˜)
2. ê³µì¢… ì»¬ëŸ¼ ë¶„í•  : ê³µì¢… ì—´ì„ '>' ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê¸° (ëŒ€ë¶„ë¥˜ or ì¤‘ë¶„ë¥˜)
3. ì‚¬ê³ ê°ì²´ ì»¬ëŸ¼ ë¶„í•  : ì‚¬ê³ ê°ì²´ ì—´ì„ '>' ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê¸° (ëŒ€ë¶„ë¥˜ or ì¤‘ë¶„ë¥˜)
```
train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data./train.csv', encoding = 'utf-8-sig')
test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data./test.csv', encoding = 'utf-8-sig')

train['ê³µì‚¬ì¢…ë¥˜(ëŒ€ë¶„ë¥˜)'] = train['ê³µì‚¬ì¢…ë¥˜'].str.split('/').str[0]
train['ê³µì‚¬ì¢…ë¥˜(ì¤‘ë¶„ë¥˜)'] = train['ê³µì‚¬ì¢…ë¥˜'].str.split('/').str[1]
train['ê³µì¢…(ëŒ€ë¶„ë¥˜)'] = train['ê³µì¢…'].str.split('>').str[0]
train['ê³µì¢…(ì¤‘ë¶„ë¥˜)'] = train['ê³µì¢…'].str.split('>').str[1]
train['ì‚¬ê³ ê°ì²´(ëŒ€ë¶„ë¥˜)'] = train['ì‚¬ê³ ê°ì²´'].str.split('>').str[0]
train['ì‚¬ê³ ê°ì²´(ì¤‘ë¶„ë¥˜)'] = train['ì‚¬ê³ ê°ì²´'].str.split('>').str[1]

test['ê³µì‚¬ì¢…ë¥˜(ëŒ€ë¶„ë¥˜)'] = test['ê³µì‚¬ì¢…ë¥˜'].str.split('/').str[0]
test['ê³µì‚¬ì¢…ë¥˜(ì¤‘ë¶„ë¥˜)'] = test['ê³µì‚¬ì¢…ë¥˜'].str.split('/').str[1]
test['ê³µì¢…(ëŒ€ë¶„ë¥˜)'] = test['ê³µì¢…'].str.split('>').str[0]
test['ê³µì¢…(ì¤‘ë¶„ë¥˜)'] = test['ê³µì¢…'].str.split('>').str[1]
test['ì‚¬ê³ ê°ì²´(ëŒ€ë¶„ë¥˜)'] = test['ì‚¬ê³ ê°ì²´'].str.split('>').str[0]
test['ì‚¬ê³ ê°ì²´(ì¤‘ë¶„ë¥˜)'] = test['ì‚¬ê³ ê°ì²´'].str.split('>').str[1]
```

### ë°ì´í„°í”„ë ˆì„ ê¸°ë°˜ QA í˜•ì‹ ë°ì´í„°ì…‹ ìƒì„±
1. í›ˆë ¨ ë°ì´í„°
- ê° í–‰ì— ëŒ€í•´ íŠ¹ì • í•„ë“œë¥¼ ì¡°í•©í•˜ì—¬ questionê³¼ answer êµ¬ì¡° ë§Œë“¤ê¸°
- question : ì‚¬ê³ ì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬
- answer : í•´ë‹¹ ì‚¬ê³ ì˜ 'ì¬ë°œë°©ì§€ëŒ€ì±… ë° í–¥í›„ì¡°ì¹˜ê³„íš' ì €ì¥
- combined_training_data : ì§ˆë¬¸ê³¼ ë‹µë³€ì´ í¬í•¨ëœ Pandas DataFrame í˜•íƒœë¡œ ë³€í™˜
```
combined_training_data = train.apply(
     lambda row: {
        "question": (
            f"ê³µì‚¬ì¢…ë¥˜ ëŒ€ë¶„ë¥˜ '{row['ê³µì‚¬ì¢…ë¥˜(ëŒ€ë¶„ë¥˜)']}', ì¤‘ë¶„ë¥˜ '{row['ê³µì‚¬ì¢…ë¥˜(ì¤‘ë¶„ë¥˜)']}' ê³µì‚¬ ì¤‘ "
            f"ê³µì¢… ëŒ€ë¶„ë¥˜ '{row['ê³µì¢…(ëŒ€ë¶„ë¥˜)']}', ì¤‘ë¶„ë¥˜ '{row['ê³µì¢…(ì¤‘ë¶„ë¥˜)']}' ì‘ì—…ì—ì„œ "
            f"ì‚¬ê³ ê°ì²´ '{row['ì‚¬ê³ ê°ì²´(ëŒ€ë¶„ë¥˜)']}'(ì¤‘ë¶„ë¥˜: '{row['ì‚¬ê³ ê°ì²´(ì¤‘ë¶„ë¥˜)']}')ì™€ ê´€ë ¨ëœ ì‚¬ê³ ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. "
            f"ì‘ì—… í”„ë¡œì„¸ìŠ¤ëŠ” '{row['ì‘ì—…í”„ë¡œì„¸ìŠ¤']}'ì´ë©°, ì‚¬ê³  ì›ì¸ì€ '{row['ì‚¬ê³ ì›ì¸']}'ì…ë‹ˆë‹¤. "
            f"ì¬ë°œ ë°©ì§€ ëŒ€ì±… ë° í–¥í›„ ì¡°ì¹˜ ê³„íšì€ ë¬´ì—‡ì¸ê°€ìš”?"
        ),
        "answer": row["ì¬ë°œë°©ì§€ëŒ€ì±… ë° í–¥í›„ì¡°ì¹˜ê³„íš"]
    },
    axis=1
)

# DataFrameìœ¼ë¡œ ë³€í™˜
combined_training_data = pd.DataFrame(list(combined_training_data))
```
   
2. í…ŒìŠ¤íŠ¸ ë°ì´í„°
- ê° í–‰ì— ëŒ€í•´ íŠ¹ì • í•„ë“œë¥¼ ì¡°í•©í•˜ì—¬ question êµ¬ì¡° ë§Œë“¤ê¸°
- question : ì‚¬ê³ ì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬
- combined_test_data : ì§ˆë¬¸ì´ í¬í•¨ëœ Pandas DataFrame í˜•íƒœë¡œ ë³€í™˜
```
combined_test_data = test.apply(
    lambda row: {
        "question": (
            f"ê³µì‚¬ì¢…ë¥˜ ëŒ€ë¶„ë¥˜ '{row['ê³µì‚¬ì¢…ë¥˜(ëŒ€ë¶„ë¥˜)']}', ì¤‘ë¶„ë¥˜ '{row['ê³µì‚¬ì¢…ë¥˜(ì¤‘ë¶„ë¥˜)']}' ê³µì‚¬ ì¤‘ "
            f"ê³µì¢… ëŒ€ë¶„ë¥˜ '{row['ê³µì¢…(ëŒ€ë¶„ë¥˜)']}', ì¤‘ë¶„ë¥˜ '{row['ê³µì¢…(ì¤‘ë¶„ë¥˜)']}' ì‘ì—…ì—ì„œ "
            f"ì‚¬ê³ ê°ì²´ '{row['ì‚¬ê³ ê°ì²´(ëŒ€ë¶„ë¥˜)']}'(ì¤‘ë¶„ë¥˜: '{row['ì‚¬ê³ ê°ì²´(ì¤‘ë¶„ë¥˜)']}')ì™€ ê´€ë ¨ëœ ì‚¬ê³ ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. "
            f"ì‘ì—… í”„ë¡œì„¸ìŠ¤ëŠ” '{row['ì‘ì—…í”„ë¡œì„¸ìŠ¤']}'ì´ë©°, ì‚¬ê³  ì›ì¸ì€ '{row['ì‚¬ê³ ì›ì¸']}'ì…ë‹ˆë‹¤. "
            f"ì¬ë°œ ë°©ì§€ ëŒ€ì±… ë° í–¥í›„ ì¡°ì¹˜ ê³„íšì€ ë¬´ì—‡ì¸ê°€ìš”?"
        )
    },
    axis=1
)

# DataFrameìœ¼ë¡œ ë³€í™˜
combined_test_data = pd.DataFrame(list(combined_test_data))
```

### ëª¨ë¸ ì„¤ì •
- BitsAndBytesConfig : Hugging Face transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” ì €ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •í´ë˜ìŠ¤(ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ, ì†ë„ í–¥ìƒ, ì„±ëŠ¥ ìœ ì§€ë¥¼ ëª©ì ìœ¼ë¡œ ì‚¬ìš©)
- load_in_4bit = True : ëª¨ë¸ì„ 4bit ì–‘ìí™”í•˜ì—¬ ë¡œë“œ(ëŒ€í˜• ëª¨ë¸ì„ ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ê¸° ìœ„í•¨).
- bnb_4bit_use_double_quant = True : ì´ì¤‘ ì–‘ìí™”(íš¨ìœ¨ì ì¸ ì••ì¶• ê°€ëŠ¥).
- bnb_4bit_quant_type = "nf4" : Normal Float 4(LLM ì„±ëŠ¥ ì €í•˜ë¥¼ ìµœì†Œí™”í•˜ë©´ì„œ ë©”ëª¨ë¦¬ ì ˆì•½ ê°€ëŠ¥).
- bnb_4bit_compute_dtype = torch.bfloat16 : NVIDIA A100, H100 ë“± ìµœì‹  GPUì— ìµœì í™”
```
bnb_config = BitsAndBytesConfig(
    load_in_4bit = True,
    bnb_4bit_use_double_quant = True,
    bnb_4bit_quant_type = "nf4",
    bnb_4bit_compute_dtype = torch.bfloat16
)
```

### LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct ëª¨ë¸
- LG AI Researchì—ì„œ ê°œë°œí•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸
- ëª¨ë¸ ì‚¬ì´ì¦ˆ : 32B, 7.8B, 2.4B
- ì¥ì  : ì €ì‚¬ì–‘ GPUì—ì„œë„ í›ˆë ¨ ë° ë°°í¬ ê°€ëŠ¥, long-context processing ê°€ëŠ¥, ìµœëŒ€ 32K í† í°ì˜ ê¸´ ë¬¸ë§¥ ì²˜ë¦¬ ê°€ëŠ¥
- í•œêµ­ì–´ ë° ì˜ì–´ ì§€ì›
```
from transformers import AutoTokenizer
from transformers import AutoModelForCausalLM

model_id = "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"

model = AutoModelForCausalLM.from_pretrained(model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id, quantization_config=bnb_config, device_map="auto")
```

### FAISS ê¸°ë°˜ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
1. í›ˆë ¨ ë°ì´í„° ì¤€ë¹„
- conbined_training_dataì—ì„œ ì§ˆë¬¸ê³¼ ë‹µë³€ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜´.
- tolist()ë¥¼ ì‚¬ìš©í•´ Pandas ì‹œë¦¬ì¦ˆë¥¼ Python ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜.
- ê° ì§ˆë¬¸ê³¼ ë‹µë³€ì„ Q&A í¬ë§·ì˜ í…ìŠ¤íŠ¸ ë¬¸ì„œë¡œ ë³€í™˜.
- zip(train_questions_prevention, train_answers_prevention) : ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ìŒìœ¼ë¡œ ì²˜ë¦¬
2. ì„ë² ë”© ëª¨ë¸ ì •ì˜
- jhgan/ko-sbert-nli : Sentence-BERT ê¸°ë°˜ì˜ í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸(=Hugging Faceì—ì„œ ì œê³µí•˜ëŠ” í•œê¸€ ë¬¸ì¥ ì˜ë¯¸ ë¶„ì„ ìµœì í™” ëª¨ë¸).
3. ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
- FAISS.from_texts() : í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë²¡í„°í™”í•˜ì—¬ FAISS ì €ì¥ì†Œì— ì¶”ê°€
- FAISS : ëŒ€ê·œëª¨ ë²¡í„° ê²€ìƒ‰ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬. ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ë¹ ë¥´ê²Œ ì°¾ê¸° ìœ„í•œ ë²¡í„° ì¸ë±ì‹± ì§€ì›.
- í›ˆë ¨ ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ FAISSì— ì €ì¥
4. Retriever ì •ì˜(ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰)
```
# vector store ìƒì„±
# Train ë°ì´í„° ì¤€ë¹„
train_questions_prevention = combined_training_data['question'].tolist()
train_answers_prevention = combined_training_data['answer'].tolist()

train_documents = [
    f"Q: {q1}\nA: {a1}"
    for q1, a1 in zip(train_questions_prevention, train_answers_prevention)
]

# ì„ë² ë”© ìƒì„±
embedding_model_name = "jhgan/ko-sbert-nli"  # ì„ë² ë”© ëª¨ë¸ ì„ íƒ
embedding = HuggingFaceEmbeddings(model_name=embedding_model_name)

# ë²¡í„° ìŠ¤í† ì–´ì— ë¬¸ì„œ ì¶”ê°€
vector_store = FAISS.from_texts(train_documents, embedding)

# Retriever ì •ì˜
retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 5})
```

